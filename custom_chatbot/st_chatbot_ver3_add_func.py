import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜´
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryMemory

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="ì»¤ìŠ¤í…€ ì±—ë´‡ í˜ì´ì§€ ì œëª©",  # ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œí•  í˜ì´ì§€ ì œëª©
    page_icon="ğŸ¤–",  # í˜ì´ì§€ íƒ­ì— í‘œì‹œí•  ì•„ì´ì½˜
    layout="centered"  # í˜ì´ì§€ ë ˆì´ì•„ì›ƒì„ ê°€ìš´ë° ì •ë ¬ë¡œ ì„¤ì •
)

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

# í˜ì´ì§€ í—¤ë” ì„¤ì •
st.title("ğŸ¤– AIë¡œ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")  # í˜ì´ì§€ íƒ€ì´í‹€ í‘œì‹œ
st.markdown("### AIì™€ í•¨ê»˜ ê¶ê¸ˆì¦ì„ í•´ê²°í•´ ë³´ì„¸ìš”. ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  **ë³´ë‚´ê¸°** ë²„íŠ¼ì„ ëˆŒëŸ¬ë³´ì„¸ìš”!")  # ì„œë¸Œ íƒ€ì´í‹€ í‘œì‹œ

# ì‘ë‹µ ë‹¤ì–‘ì„± ì„¤ì • ìŠ¬ë¼ì´ë” ì¶”ê°€ (temperature ì¡°ì •)
temperature = st.slider("ì‘ë‹µ ë‹¤ì–‘ì„± ì„¤ì • (Temperature)", 0.0, 1.0, 0.7, step=0.05)

# ëŒ€í™” ìŠ¤íƒ€ì¼ ì„ íƒ ì˜µì…˜ ì¶”ê°€
style = st.selectbox("ëŒ€í™” ìŠ¤íƒ€ì¼ ì„ íƒ", ("ì •ì¤‘í•œ", "ì¼ìƒì ", "ì „ë¬¸ì "))

# ChatOpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± ë° ë©”ëª¨ë¦¬ ì¶”ê°€
def generate_response(input_text): 
    # ChatOpenAI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = ChatOpenAI(
        temperature=temperature,  # ì‚¬ìš©ìê°€ ì„¤ì •í•œ ì‘ë‹µ ë‹¤ì–‘ì„± ì ìš©
        model_name='gpt-4o-mini'  # ì‚¬ì „ ì§€ì •ëœ AI ëª¨ë¸ ì‚¬ìš©
    )
    
    # ConversationSummaryMemory ì„¤ì • (í† í° í¬ê¸° ì œí•œ)
    if "memory" not in st.session_state:
        st.session_state["memory"] = ConversationSummaryMemory(llm=llm, max_token_limit=500)  # ìµœëŒ€ 500 í† í°ìœ¼ë¡œ ì œí•œ
    
    # ëŒ€í™” ìŠ¤íƒ€ì¼ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    style_prefix = {
        "ì •ì¤‘í•œ": "ì •ì¤‘í•˜ê²Œ",
        "ì¼ìƒì ": "í¸í•˜ê²Œ",
        "ì „ë¬¸ì ": "ì „ë¬¸ì ìœ¼ë¡œ"
    }
    style_instruction = style_prefix[style]
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ê°€ì ¸ì™€ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜
    memory_summary = st.session_state["memory"].load_memory_variables({})["history"]
    full_input = f"{memory_summary}\n[{style_instruction}] User: {input_text}\nAI:"
    response = llm.predict(full_input)  # ì‚¬ìš©ì ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ AI ì‘ë‹µ ìƒì„±
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
    st.session_state["memory"].save_context({"input": input_text}, {"output": response})
    
    return response  # ì‘ë‹µ ë°˜í™˜

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ” í¼ ìƒì„± (ë§¨ ì•„ë˜ì— ì…ë ¥ì°½ ìœ ì§€)
text = st.text_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: OpenAIì˜ í…ìŠ¤íŠ¸ ëª¨ë¸ ì¢…ë¥˜ëŠ” ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?")  # í…ìŠ¤íŠ¸ ì…ë ¥ í•„ë“œ ìƒì„±
submitted = st.button("ë³´ë‚´ê¸°")  # 'ë³´ë‚´ê¸°' ë²„íŠ¼ ìƒì„±í•˜ì—¬ ì œì¶œ

# ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì œì¶œí–ˆì„ ë•Œ ë‹µë³€ ìƒì„±
if submitted and text:  # í¼ì´ ì œì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì…ë ¥ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
    response = generate_response(text)  # ì‘ë‹µ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    st.session_state["chat_history"].append(("User", text))  # ì‚¬ìš©ì ì…ë ¥ì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    st.session_state["chat_history"].append(("AI", response))  # AI ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ì…ë ¥ì°½ ìœ„ë¡œ ì¶œë ¥ë˜ë„ë¡ ìˆ˜ì •)
chat_history_reversed = st.session_state["chat_history"][::-1]  # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì—­ìˆœìœ¼ë¡œ ë³€ê²½
for speaker, message in chat_history_reversed:
    if speaker == "User":
        st.write(f"**User:** {message}")
    else:
        st.info(f"**AI:** {message}")

# í‘¸í„° ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
st.markdown("---")  # êµ¬ë¶„ì„ ì„ ì¶”ê°€í•˜ì—¬ í˜ì´ì§€ í•˜ë‹¨ì„ ë¶„ë¦¬
st.caption("Powered by LangChain & OpenAI API")  # í˜ì´ì§€ í•˜ë‹¨ì— APIì™€ íˆ´ ì¶œì²˜ í‘œì‹œ
