import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜´
from langchain_openai import ChatOpenAI
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.document_loaders import PyPDFLoader
import os
import tempfile

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="AI ê¸°ë°˜ ê³ ê° ì‘ëŒ€ ì„œë¹„ìŠ¤",  # ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œí•  í˜ì´ì§€ ì œëª©
    page_icon="ğŸŒ",  # í˜ì´ì§€ íƒ­ì— í‘œì‹œí•  ì§€êµ¬ë³¸ ì•„ì´ì½˜
    layout="centered"  # í˜ì´ì§€ ë ˆì´ì•„ì›ƒì„ ì¤‘ì•™ì— ì •ë ¬í•˜ì—¬ ëª¨ë°”ì¼ ì±„íŒ… ìŠ¤íƒ€ì¼ë¡œ êµ¬ì„±
)

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

if "memory" not in st.session_state:
    st.session_state["memory"] = []

if 'retriever' not in st.session_state:
    # ì‚¬ì „ì— ì •ì˜ëœ ë¬¸ì„œ ê²½ë¡œ ëª©ë¡
    document_paths = ["Auto_QnA/docs/ma2155_iphone-15-info.pdf", 
                      "Auto_QnA/docs/iPhone ìˆ˜ë¦¬ ë° ì„œë¹„ìŠ¤ - Apple ì§€ì› (KR).pdf"] # ì—¬ê¸°ì„œ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì—¬ ë°±ì—”ë“œì—ì„œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´
    all_documents = []
    for path in document_paths:
        if os.path.exists(path):
            loader = PyPDFLoader(path)
            data = loader.load()
            all_documents.extend(data)

    embeddings = OpenAIEmbeddings()
    vectors = FAISS.from_documents(all_documents, embeddings)
    st.session_state['retriever'] = vectors.as_retriever()

    # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
    text_content = ""
    char_limit = 5000  # í† í° ì œí•œì„ ìœ„í•œ ë¬¸ì ìˆ˜ ì œí•œ
    current_chars = 0
    for doc in all_documents:
        page_text = doc.page_content
        if current_chars + len(page_text) > char_limit:
            text_content += page_text[:char_limit - current_chars]
            break
        else:
            text_content += page_text + "\n"
            current_chars += len(page_text)

    # ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
    def generate_recommended_questions(text_content):
        llm = ChatOpenAI(
            temperature=0.7,
            model_name='gpt-4o-mini'
        )
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìê°€ í•  ë§Œí•œ ì¶”ì²œ ì§ˆë¬¸ 5ê°œë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

{text_content}

ì¶”ì²œ ì§ˆë¬¸:"""
        response = llm.predict(prompt)
        questions = response.strip().split('\n')
        # ì§ˆë¬¸ ëª©ë¡ ì •ë¦¬
        clean_questions = [q.strip('1234567890. ').strip() for q in questions if q.strip()]
        # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        clean_questions = clean_questions[:5]
        return clean_questions

    recommended_questions = generate_recommended_questions(text_content)
    st.session_state['recommended_questions'] = recommended_questions

# ìƒë‹¨ í—¤ë” ì„¤ì •
st.markdown("""
    <div style="background-color:#4A90E2;padding:10px;border-radius:10px;">
        <h1 style="color:white;text-align:center;"> ğŸ¤– AI ê³ ê° ì‘ë‹µ ì„œë¹„ìŠ¤</h1>
    </div>
""", unsafe_allow_html=True)  # HTMLë¡œ ìƒë‹¨ í—¤ë” ì˜ì—­ì˜ ë°°ê²½ê³¼ ìƒ‰ìƒ ìŠ¤íƒ€ì¼ë§

# ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ (ì„¸ì…˜ì— ì €ì¥ëœ ê²½ìš°ì—ë§Œ í‘œì‹œ)
if 'recommended_questions' in st.session_state and st.session_state['recommended_questions']:
    st.markdown("### ì¶”ì²œ ì§ˆë¬¸")
    for idx, question in enumerate(st.session_state['recommended_questions'], 1):
        st.write(f"{idx}. {question}")

# ì±„íŒ… ìŠ¤íƒ€ì¼ì˜ UI êµ¬ì„±
# ChatOpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± ë° ë©”ëª¨ë¦¬ ì¶”ê°€
def generate_response(input_text): 
    # ChatOpenAI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = ChatOpenAI(
        temperature=1.0,  # ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ìµœì†Œí™”í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ ì œê³µ
        model_name='gpt-4o-mini'  # ì‚¬ì „ ì§€ì •ëœ AI ëª¨ë¸ ì‚¬ìš©
    )
    
    if 'retriever' in st.session_state:
        # RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=st.session_state['retriever'],
            return_source_documents=False
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
        result = chain({
            "question": input_text,
            "chat_history": []  # í˜¸ì¶œì‹œ ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ê°€ì§€ê³  ë‹µë³€ ìƒì„±
        })
        response = result['answer']
    else:
        # ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
        memory_summary = "\n".join(
            [f"User: {q}\nAI: {a}" for q, a in st.session_state['chat_history']]
        )
        full_input = f"{memory_summary}\nUser: {input_text}\nAI:"
        response = llm.predict(full_input)
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥
    st.session_state['chat_history'].append(("User", input_text))
    st.session_state['chat_history'].append(("AI", response))
    
    return response  # ì‘ë‹µ ë°˜í™˜

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ” í´ ìƒì„±
with st.form("question_form"):  # í´ì„ ìƒì„±í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ìŒ
    text = st.text_input(
        "ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:", 
        placeholder="ì˜ˆ: ì•„ì´í° ê¸°ëŠ¥ì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    )  # í…ìŠ¤íŠ¸ ì…ë ¥ í•„ë“œ ìƒì„±
    submitted = st.form_submit_button("ë³´ë‚´ê¸°", use_container_width=True)  # 'ë³´ë‚´ê¸°' ë²„íŠ¼ ìƒì„±í•˜ì—¬ í´ ì œì¶œ

    # ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì œì¶œí–ˆì„ ë•Œ ë‹µë³€ ìƒì„±
    if submitted and text.strip():
        response = generate_response(text)  # ì‘ë‹µ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    elif submitted:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ìµœì‹  ëŒ€í™”ê°€ ìœ„ë¡œ ì¶œë ¥ë˜ë„ë¡ ì„¤ì •)
chat_history_reversed = st.session_state["chat_history"][::-1]  # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì—­ìˆœìœ¼ë¡œ ë³€ê²½
for speaker, message in chat_history_reversed:
    if speaker == "User":
        st.markdown(f"""
            <div style="background-color:#DCF8C6;padding:10px;border-radius:10px;margin-top:10px;width:80%;margin-left:auto;">
                <p><strong>ê³ ê°ë‹˜:</strong> {message}</p>
            </div>
        """, unsafe_allow_html=True)  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìŠ¤íƒ€ì¼ë§í•˜ì—¬ í‘œì‹œ
    else:
        st.markdown(f"""
            <div style="background-color:#F0F0F0;padding:10px;border-radius:10px;margin-top:10px;width:80%;">
                <p><strong>ğŸ’¡ AI ì‘ë‹µ:</strong> {message}</p>
            </div>
        """, unsafe_allow_html=True)  # AI ì‘ë‹µì„ ìŠ¤íƒ€ì¼ë§í•˜ì—¬ í‘œì‹œ

# í‘¸í„° ì¶”ê°€
st.markdown("""
    <div style="margin-top: 50px; padding: 15px; background-color: #333; color: white; text-align: center; border-radius: 10px;">
        <p>Powered by LangChain & OpenAI API | ğŸ¦ ê³ ê°ì˜ ë§Œì¡±ì„ ìœ„í•´ ì–¸ì œë‚˜ ìµœì„ ì„ ë‹¤í•©ë‹ˆë‹¤.</p>
    </div>
""", unsafe_allow_html=True)  # í˜ì´ì§€ í•˜ë‹¨ì— í‘¸í„° í‘œì‹œ (ìŠ¤íƒ€ì¼ë§ ì ìš©)